{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark-NLP and Spark ML Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Topic Modeling\n",
    "\n",
    "`Spark-NLP`\n",
    "* DocumentAssembler\n",
    "* SentenceDetector\n",
    "* Tokenizer\n",
    "* Normalizer\n",
    "* POS tagger\n",
    "* Chunker\n",
    "* Finisher\n",
    "\n",
    "`Spark ML`\n",
    "* Hashing\n",
    "* TF-IDF\n",
    "* LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import CountVectorizer, HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.clustering import LDA, LDAModel\n",
    "\n",
    "#Spark NLP\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import RegexRule\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a Spark Session for our app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download some scientific sample from PubMed dataset:\n",
    "```\n",
    "wget -N \thttps://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/pubmed/pubmed-sample.csv -P /tmp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-23 20:02:11--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/pubmed/pubmed-sample.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.18.11\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.18.11|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘/tmp/pubmed-sample.csv’ not modified on server. Omitting download.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -N \thttps://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/pubmed/pubmed-sample.csv -P /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubMedDF = spark.read\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .csv(\"/tmp/pubmed-sample.csv\")\\\n",
    "                .filter(\"AB IS NOT null\")\\\n",
    "                .withColumn(\"text\", col(\"AB\"))\\\n",
    "                .drop(\"TI\", \"AB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|The human KCNJ9 (...|\n",
      "|BACKGROUND: At pr...|\n",
      "|OBJECTIVE: To inv...|\n",
      "|Combined EEG/fMRI...|\n",
      "|Kohlschutter synd...|\n",
      "|Statistical analy...|\n",
      "|The synthetic DOX...|\n",
      "|Our objective was...|\n",
      "|We conducted a ph...|\n",
      "|\"Monomeric sarcos...|\n",
      "|We presented the ...|\n",
      "|The literature de...|\n",
      "|A novel approach ...|\n",
      "|An HPLC-ESI-MS-MS...|\n",
      "|The localizing an...|\n",
      "|OBJECTIVE: To eva...|\n",
      "|For the construct...|\n",
      "|We report the res...|\n",
      "|Intraparenchymal ...|\n",
      "|It is known that ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pubMedDF.printSchema()\n",
    "pubMedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7537"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubMedDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create Spark-NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.2 ms, sys: 5.51 ms, total: 20.7 ms\n",
      "Wall time: 5.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Spark-NLP Pipeline\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "  .setInputCols([\"token\"]) \\\n",
    "  .setOutputCol(\"normalized\") \\\n",
    "  .setLowercase(True)\n",
    "\n",
    "posTagger = PerceptronModel.pretrained() \\\n",
    "  .setInputCols([\"sentence\", \"normalized\"])\n",
    "\n",
    "chunker = Chunker() \\\n",
    "    .setInputCols([\"sentence\", \"pos\"]) \\\n",
    "    .setOutputCol(\"chunk\") \\\n",
    "    .setRegexParsers([\"<NNP>+\", \"<DT|PP\\\\$>?<JJ>*<NN>\"])\n",
    "\n",
    "finisher = Finisher() \\\n",
    "  .setInputCols([\"chunk\"]) \\\n",
    "  .setIncludeMetadata(False)\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    sentence_detector, \n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    posTagger,\n",
    "    chunker,\n",
    "    finisher\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.6 ms, sys: 17.4 ms, total: 74 ms\n",
      "Wall time: 749 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlpPipelineDF = nlpPipeline.fit(pubMedDF).transform(pubMedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create Spark ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.45 ms, sys: 3.45 ms, total: 8.89 ms\n",
      "Wall time: 52 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SPark ML Pipeline\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"finished_chunk\", outputCol=\"features\", vocabSize=3000, minDF=10.0)\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"idf\")\n",
    "lda = LDA(k=10, maxIter=1)\n",
    "### Let's create Spark-NLP Pipeline\n",
    "mlPipeline = Pipeline(stages=[\n",
    "    cv,\n",
    "    idf,\n",
    "    lda\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to train Spark ML Pipeline by using Spark-NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's create Spark-NLP Pipeline%%time\n",
    "mlModel = mlPipeline.fit(nlpPipelineDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15 ms, sys: 7.55 ms, total: 22.5 ms\n",
      "Wall time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mlPipelineDF = mlModel.transform(nlpPipelineDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|      finished_chunk|            features|                 idf|   topicDistribution|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|The human KCNJ9 (...|[The human KCNJ, ...|(3000,[6,12,28,72...|(3000,[6,12,28,72...|[0.00368146035207...|\n",
      "|BACKGROUND: At pr...|[BACKGROUND, the ...|(3000,[0,4,14,15,...|(3000,[0,4,14,15,...|[0.44577090261779...|\n",
      "|OBJECTIVE: To inv...|[OBJECTIVE, the r...|(3000,[7,19,44,56...|(3000,[7,19,44,56...|[0.00310604411566...|\n",
      "|Combined EEG/fMRI...|[EEG/fMR, recordi...|(3000,[13,24,46,1...|(3000,[13,24,46,1...|[0.00473349830410...|\n",
      "|Kohlschutter synd...|[Kohlschutter, sy...|(3000,[110,645,90...|(3000,[110,645,90...|[0.02488645895143...|\n",
      "|Statistical analy...|[Statistical anal...|(3000,[9,32,60,19...|(3000,[9,32,60,19...|[0.00764828599675...|\n",
      "|The synthetic DOX...|[The synthetic DO...|(3000,[2,42,77,11...|(3000,[2,42,77,11...|[0.00552317357383...|\n",
      "|Our objective was...|[objective, blood...|(3000,[0,1,4,13,3...|(3000,[0,1,4,13,3...|[0.00168468598037...|\n",
      "|We conducted a ph...|[a phase, II, stu...|(3000,[9,13,72,96...|(3000,[9,13,72,96...|[0.00320605148744...|\n",
      "|\"Monomeric sarcos...|[Monomeric sarcos...|(3000,[1,71,93,11...|(3000,[1,71,93,11...|[0.46831578556725...|\n",
      "|We presented the ...|[the tachinid, fl...|(3000,[22,31,89,2...|(3000,[22,31,89,2...|[0.00397587737105...|\n",
      "|The literature de...|[The literature, ...|(3000,[13,79,81,3...|(3000,[13,79,81,3...|[0.00497033216680...|\n",
      "|A novel approach ...|[A novel, approac...|(3000,[25,39,91,1...|(3000,[25,39,91,1...|[0.00903933699678...|\n",
      "|An HPLC-ESI-MS-MS...|[I, I, method, th...|(3000,[0,10,46,54...|(3000,[0,10,46,54...|[0.68208426305967...|\n",
      "|The localizing an...|[eye, head, durin...|(3000,[378,400,43...|(3000,[378,400,43...|[0.00828581450286...|\n",
      "|OBJECTIVE: To eva...|[June, OBJECTIVE,...|(3000,[3,14,21,23...|(3000,[3,14,21,23...|[0.00414191890172...|\n",
      "|For the construct...|[the construction...|(3000,[125,258,56...|(3000,[125,258,56...|[0.01658242294401...|\n",
      "|We report the res...|[a screen, geneti...|(3000,[12,17,81,2...|(3000,[12,17,81,2...|[0.00764821353361...|\n",
      "|Intraparenchymal ...|[Intraparenchymal...|(3000,[4,24,28,64...|(3000,[4,24,28,64...|[0.00621357103062...|\n",
      "|It is known that ...|[Klinefelter, syn...|(3000,[2,110,400,...|(3000,[2,110,400,...|[0.85089612500550...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlPipelineDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -1427616.1489268562\n",
      "The upper bound on perplexity: 8.376505148282039\n",
      "CPU times: user 44.1 ms, sys: 38.4 ms, total: 82.5 ms\n",
      "Wall time: 6min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ldaModel = mlModel.stages[2]\n",
    "\n",
    "ll = ldaModel.logLikelihood(mlPipelineDF)\n",
    "lp = ldaModel.logPerplexity(mlPipelineDF)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics described by their top-weighted terms:\n",
      "+-----+-------------+---------------------------------------------------------------------+\n",
      "|topic|termIndices  |termWeights                                                          |\n",
      "+-----+-------------+---------------------------------------------------------------------+\n",
      "|0    |[0, 2, 5]    |[0.013919034901292178, 0.006700064793232536, 0.003015485586985393]   |\n",
      "|1    |[1, 2, 77]   |[0.0034154322431057243, 0.0027432705547898095, 0.0020877414099877895]|\n",
      "|2    |[108, 25, 8] |[0.004633856073583384, 0.0023776985723297345, 0.002026254388192705]  |\n",
      "|3    |[7, 6, 45]   |[0.003100067737830986, 0.003057422598986175, 0.002382902473412686]   |\n",
      "|4    |[59, 14, 211]|[0.0024608783220347666, 0.0020297154176113497, 0.0018962959098669185]|\n",
      "|5    |[111, 36, 0] |[0.0020101336462628257, 0.0019800534802662255, 0.001965065438641306] |\n",
      "|6    |[22, 46, 6]  |[0.002519452910460417, 0.0023172448672150424, 0.0022960831624377676] |\n",
      "|7    |[1, 7, 3]    |[0.005489733337879858, 0.0027567556191902634, 0.002169743290819701]  |\n",
      "|8    |[48, 4, 17]  |[0.003867867446315102, 0.0036077453941906934, 0.003532508131551274]  |\n",
      "|9    |[4, 47, 14]  |[0.0031362806773423146, 0.0018813408848740252, 0.0018677970936964246]|\n",
      "+-----+-------------+---------------------------------------------------------------------+\n",
      "\n",
      "CPU times: user 3.55 ms, sys: 5.13 ms, total: 8.68 ms\n",
      "Wall time: 194 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Describe topics.\n",
    "topics = ldaModel.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned topics (as distributions over vocab of 3000 words):\n",
      "Topic 0:DenseMatrix([[50.73783652,  5.04405936,  0.78578643, ...,  3.81858649,\n",
      "               8.53652028,  0.69201923],\n",
      "             [ 9.76344508, 11.50356713,  2.16274875, ..., 18.54846812,\n",
      "               4.93117058,  5.15730655],\n",
      "             [24.42315825,  9.23964955,  1.35829732, ...,  0.92189725,\n",
      "               3.79467484,  2.62953158],\n",
      "             ...,\n",
      "             [ 0.94607864,  0.90747457,  1.16281851, ...,  0.9498116 ,\n",
      "               0.8931674 ,  1.03045281],\n",
      "             [ 0.91667884,  0.99775866,  0.83649044, ...,  0.97774381,\n",
      "               0.93629883,  0.91163679],\n",
      "             [ 1.07634159,  1.45076257,  0.86056615, ...,  1.01649731,\n",
      "               1.1209169 ,  1.02465341]])\n",
      "Topic 1:DenseMatrix([[50.73783652,  5.04405936,  0.78578643, ...,  3.81858649,\n",
      "               8.53652028,  0.69201923],\n",
      "             [ 9.76344508, 11.50356713,  2.16274875, ..., 18.54846812,\n",
      "               4.93117058,  5.15730655],\n",
      "             [24.42315825,  9.23964955,  1.35829732, ...,  0.92189725,\n",
      "               3.79467484,  2.62953158],\n",
      "             ...,\n",
      "             [ 0.94607864,  0.90747457,  1.16281851, ...,  0.9498116 ,\n",
      "               0.8931674 ,  1.03045281],\n",
      "             [ 0.91667884,  0.99775866,  0.83649044, ...,  0.97774381,\n",
      "               0.93629883,  0.91163679],\n",
      "             [ 1.07634159,  1.45076257,  0.86056615, ...,  1.01649731,\n",
      "               1.1209169 ,  1.02465341]])\n",
      "Topic 2:DenseMatrix([[50.73783652,  5.04405936,  0.78578643, ...,  3.81858649,\n",
      "               8.53652028,  0.69201923],\n",
      "             [ 9.76344508, 11.50356713,  2.16274875, ..., 18.54846812,\n",
      "               4.93117058,  5.15730655],\n",
      "             [24.42315825,  9.23964955,  1.35829732, ...,  0.92189725,\n",
      "               3.79467484,  2.62953158],\n",
      "             ...,\n",
      "             [ 0.94607864,  0.90747457,  1.16281851, ...,  0.9498116 ,\n",
      "               0.8931674 ,  1.03045281],\n",
      "             [ 0.91667884,  0.99775866,  0.83649044, ...,  0.97774381,\n",
      "               0.93629883,  0.91163679],\n",
      "             [ 1.07634159,  1.45076257,  0.86056615, ...,  1.01649731,\n",
      "               1.1209169 ,  1.02465341]])\n"
     ]
    }
   ],
   "source": [
    "# Output topics. Each is a distribution over words (matching word count vectors)\n",
    "print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize())\n",
    "      + \" words):\")\n",
    "\n",
    "topics = ldaModel.topicsMatrix()\n",
    "\n",
    "for topic in range(3):\n",
    "    print(\"Topic \" + str(topic) + \":\" + str(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
