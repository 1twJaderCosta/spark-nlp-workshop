{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-25 10:33:16--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sarcasm/train-balanced-sarcasm.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.178.85\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.178.85|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 255268960 (243M) [text/csv]\n",
      "Saving to: ‘/tmp/train-balanced-sarcasm.csv’\n",
      "\n",
      "train-balanced-sarc 100%[===================>] 243.44M  10.9MB/s    in 19s     \n",
      "\n",
      "2019-03-25 10:33:35 (12.6 MB/s) - ‘/tmp/train-balanced-sarcasm.csv’ saved [255268960/255268960]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sarcasm/train-balanced-sarcasm.csv -P /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "import time\n",
    "\n",
    "packages = [\n",
    "    'JohnSnowLabs:spark-nlp:2.0.1'\n",
    "]\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ML SQL session\") \\\n",
    "    .config('spark.jars.packages', ','.join(packages)) \\\n",
    "    .config('spark.executor.instances','4') \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\",\"16g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- ups: string (nullable = true)\n",
      " |-- downs: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- created_utc: string (nullable = true)\n",
      " |-- parent_comment: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=1010826)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sql = SQLContext(spark)\n",
    "\n",
    "trainBalancedSarcasmDF = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/tmp/train-balanced-sarcasm.csv\")\n",
    "trainBalancedSarcasmDF.printSchema()\n",
    "\n",
    "# Let's create a temp view (table) for our SQL queries\n",
    "trainBalancedSarcasmDF.createOrReplaceTempView('data')\n",
    "\n",
    "sql.sql('SELECT COUNT(*) FROM data').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|             comment|\n",
      "+-----+--------------------+\n",
      "|    0|Yeah, I get that ...|\n",
      "|    0|The blazers and M...|\n",
      "|    0|They're favored t...|\n",
      "|    0|deadass don't kil...|\n",
      "|    0|Yep can confirm I...|\n",
      "|    0|do you find arian...|\n",
      "|    0|What's your weird...|\n",
      "|    0|Probably Sephirot...|\n",
      "|    0|What to upgrade? ...|\n",
      "|    0|Probably count Ka...|\n",
      "|    0|I bet if that mon...|\n",
      "|    0|James Shields Wil...|\n",
      "|    0|There's no time t...|\n",
      "|    0|Team Specific Thr...|\n",
      "|    0|Ill give you a hi...|\n",
      "|    0|Star Wars, easy. ...|\n",
      "|    0|You're adorable.\n",
      "...|\n",
      "|    0|He actually acts ...|\n",
      "|    0|Clinton struggles...|\n",
      "|    0|Is that the Older...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sql.sql('select label,concat(parent_comment,\"\\n\",comment) as comment from data where comment is not null and parent_comment is not null limit 100000')\n",
    "print(type(df))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|             comment|             ntokens|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|Yeah, I get that ...|[yeah, i, get, th...|\n",
      "|    0|The blazers and M...|[the, blazer, and...|\n",
      "|    0|They're favored t...|[thei, re, favor,...|\n",
      "|    0|deadass don't kil...|[deadass, do, nt,...|\n",
      "|    0|Yep can confirm I...|[yep, can, confir...|\n",
      "|    0|do you find arian...|[do, you, find, a...|\n",
      "|    0|What's your weird...|[what, your, weir...|\n",
      "|    0|Probably Sephirot...|[probabl, sephiro...|\n",
      "|    0|What to upgrade? ...|[what, to, upgrad...|\n",
      "|    0|Probably count Ka...|[probabl, count, ...|\n",
      "|    0|I bet if that mon...|[i, bet, if, that...|\n",
      "|    0|James Shields Wil...|[jame, shield, wi...|\n",
      "|    0|There's no time t...|[there, no, time,...|\n",
      "|    0|Team Specific Thr...|[team, specif, th...|\n",
      "|    0|Ill give you a hi...|[ill, give, you, ...|\n",
      "|    0|Star Wars, easy. ...|[star, war, easi,...|\n",
      "|    0|You're adorable.\n",
      "...|[you, re, ador, n...|\n",
      "|    0|He actually acts ...|[he, actual, act,...|\n",
      "|    0|Clinton struggles...|[clinton, struggl...|\n",
      "|    0|Is that the Older...|[i, that, the, ol...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 280 ms, sys: 116 ms, total: 395 ms\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"comment\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\") \\\n",
    "    .setUseAbbreviations(True)\n",
    "    \n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"sentence\"]) \\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "    \n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"stem\"]) \\\n",
    "    .setOutputCol(\"normalized\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCols([\"ntokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(True)\n",
    "\n",
    "nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, stemmer, normalizer, finisher])\n",
    "nlp_model = nlp_pipeline.fit(df)\n",
    "processed = nlp_model.transform(df).persist()\n",
    "processed.count()\n",
    "processed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70136\n",
      "29864\n"
     ]
    }
   ],
   "source": [
    "train, test = processed.randomSplit(weights=[0.7, 0.3], seed=123)\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|             comment|             ntokens|        clean_tokens|            text_vec|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|              !\n",
      "Goes|               [goe]|               [goe]|[-0.1480705887079...|[-0.1480705887079...|\n",
      "|    0|!completed\n",
      "!compl...|  [complet, complet]|  [complet, complet]|[-0.0071502337232...|[-0.0071502337232...|\n",
      "|    0|\"\"\" \"\"Very Right ...|[veri, right, win...|[veri, right, win...|[-0.1590647930279...|[-0.1590647930279...|\n",
      "|    0|\"\"\" Perhaps you n...|[perhap, you, ne,...|[perhap, ne, stro...|[-0.0207198142095...|[-0.0207198142095...|\n",
      "|    0|\"\"\" This covering...|[thi, cover, not,...|[thi, cover, onli...|[-0.0748528920636...|[-0.0748528920636...|\n",
      "|    0|\"\"\"*Kirk\n",
      "I am sin...|[kirk, i, am, sin...|[kirk, singl, gue...|[-0.0922344097867...|[-0.0922344097867...|\n",
      "|    0|\"\"\"*looks at hand...|[look, at, hand, ...|[look, hand, doe,...|[-0.0999431064209...|[-0.0999431064209...|\n",
      "|    0|\"\"\"+100\"\" indicat...|[+, indic, come, ...|[+, indic, come, ...|[-0.0572623052220...|[-0.0572623052220...|\n",
      "|    0|\"\"\".$witty_remark...|[wittyremark, shi...|[wittyremark, shi...|[0.0,0.0,0.0,0.0,...|          (50,[],[])|\n",
      "|    0|\"\"\"... and Fancy ...|[and, fanci, feas...|[fanci, feast, so...|[-0.0938063922027...|[-0.0938063922027...|\n",
      "|    0|\"\"\"...and then th...|[and, then, the, ...|[entir, food, cou...|[-0.1649170034268...|[-0.1649170034268...|\n",
      "|    0|\"\"\"...newtons.\"\" ...|[newtons, which, ...|[newtons, nt, get...|[-0.0881061221339...|[-0.0881061221339...|\n",
      "|    0|\"\"\"100 level and ...|[level, and, k, e...|[level, k, easfc,...|[-0.0341201046636...|[-0.0341201046636...|\n",
      "|    0|\"\"\"8 operators.\"\"...|[oper, well, i, m...|[oper, well, mean...|[-0.0237525349321...|[-0.0237525349321...|\n",
      "|    0|\"\"\"@wikileaks - A...|[wikileak, americ...|[wikileak, americ...|[-0.1612923810258...|[-0.1612923810258...|\n",
      "|    0|\"\"\"A Cyborg... Ni...|[a, cyborg, ninja...|[cyborg, ninja, r...|[-0.0895159610414...|[-0.0895159610414...|\n",
      "|    0|\"\"\"A Victoria's S...|[a, victoria, sec...|[victoria, secret...|[-0.1477242468569...|[-0.1477242468569...|\n",
      "|    0|\"\"\"A basic aspect...|[a, basic, aspect...|[basic, aspect, f...|[-0.0858297945161...|[-0.0858297945161...|\n",
      "|    0|\"\"\"A sense of pur...|[a, sens, of, pur...|[sens, purpos, ve...|[-0.1008434694260...|[-0.1008434694260...|\n",
      "|    0|\"\"\"Agreed. I thin...|[agr, i, think, w...|[agr, think, issu...|[-0.1220090234571...|[-0.1220090234571...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 166 ms, sys: 63.3 ms, total: 229 ms\n",
      "Wall time: 48 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import feature as spark_ft\n",
    "\n",
    "stopWords = spark_ft.StopWordsRemover.loadDefaultStopWords('english')\n",
    "sw_remover = spark_ft.StopWordsRemover(inputCol='ntokens', outputCol='clean_tokens', stopWords=stopWords)\n",
    "text2vec = spark_ft.Word2Vec(\n",
    "    vectorSize=50, minCount=5, seed=123, \n",
    "    inputCol='ntokens', outputCol='text_vec', \n",
    "    windowSize=5, maxSentenceLength=30\n",
    ")\n",
    "assembler = spark_ft.VectorAssembler(inputCols=['text_vec'], outputCol='features')\n",
    "feature_pipeline = Pipeline(stages=[sw_remover, text2vec,assembler])\n",
    "feature_model = feature_pipeline.fit(train)\n",
    "\n",
    "train_featurized = feature_model.transform(train).persist()\n",
    "train_featurized.count()\n",
    "train_featurized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.9 ms, sys: 30.2 ms, total: 54.1 ms\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import classification as spark_cls\n",
    "\n",
    "\n",
    "mlpc = spark_cls.MultilayerPerceptronClassifier(\n",
    "    maxIter=100, seed=123, layers=[50, 25, 10,2]\n",
    ")\n",
    "\n",
    "model = mlpc.fit(train_featurized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|             comment|             ntokens|        clean_tokens|            text_vec|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    0|!RemindMe 1 week\n",
      "...|[remindm, week, r...|[remindm, week, r...|[0.11306838598102...|[0.11306838598102...|[0.89789200131038...|[0.77982102193697...|       0.0|\n",
      "|    0|!Remindme 2 weeks...|[remindm, week, r...|[remindm, week, r...|[0.22176272049546...|[0.22176272049546...|[1.01256287699695...|[0.84369743152748...|       0.0|\n",
      "|    0|!SH!TPOST!: All t...|[shtpost, all, th...|[shtpost, poor, u...|[-0.1059518517660...|[-0.1059518517660...|[0.12477470407395...|[0.39505276118435...|       1.0|\n",
      "|    0|\"\"\"**FUCK** Cloud...|[fuck, cloud, lin...|[fuck, cloud, lin...|[-0.1259684927968...|[-0.1259684927968...|[0.78120746131570...|[0.65379348980651...|       0.0|\n",
      "|    0|\"\"\"*Komrad\n",
      "\"*\"\"Th...|[komrad, those, w...|[komrad, prousa, ...|[-0.0573314118332...|[-0.0573314118332...|[-0.0689217166593...|[0.32851367955219...|       1.0|\n",
      "|    0|\"\"\"... thanks to ...|[thank, to, a, pa...|[thank, parad, tr...|[-0.1206339780921...|[-0.1206339780921...|[0.22711633926411...|[0.44023859170473...|       1.0|\n",
      "|    0|\"\"\"...FUCK IS THA...|[fuck, i, tha, de...|[fuck, tha, death...|[-0.0941429328654...|[-0.0941429328654...|[0.51861911930094...|[0.56073691372166...|       0.0|\n",
      "|    0|\"\"\"...I'm Going T...|[i, m, go, to, en...|[m, go, end, drea...|[-0.1122944690952...|[-0.1122944690952...|[0.48395140849415...|[0.54796932404146...|       0.0|\n",
      "|    0|\"\"\"A SMALL FUCKIN...|[a, small, fuck, ...|[small, fuck, hol...|[-0.2074345483311...|[-0.2074345483311...|[0.73736817992515...|[0.63092371995075...|       0.0|\n",
      "|    0|\"\"\"A new brick wa...|[a, new, brick, w...|[new, brick, wall...|[-0.0626822641185...|[-0.0626822641185...|[0.80769348433471...|[0.67927263307840...|       0.0|\n",
      "|    0|\"\"\"Add dabbing to...|[add, dab, to, mi...|[add, dab, minecr...|[-0.0252210604293...|[-0.0252210604293...|[0.88222085140941...|[0.76201971824228...|       0.0|\n",
      "|    0|\"\"\"All according ...|[all, accord, to,...|[accord, keikaku,...|[0.01586276541153...|[0.01586276541153...|[0.54604863854538...|[0.54826277013009...|       0.0|\n",
      "|    0|\"\"\"An unmet playe...|[an, unmet, playe...|[unmet, player, h...|[-0.0756642961793...|[-0.0756642961793...|[0.65238239994735...|[0.60464837250287...|       0.0|\n",
      "|    0|\"\"\"And bacon. Lot...|[and, bacon, lot,...|[bacon, lot, lot,...|[-0.0540238446556...|[-0.0540238446556...|[0.53388117800540...|[0.55006304835167...|       0.0|\n",
      "|    0|\"\"\"And later... S...|[and, later, some...|[later, someth, f...|[-0.1515038096811...|[-0.1515038096811...|[0.89035313012261...|[0.71768232117974...|       0.0|\n",
      "|    0|\"\"\"And please tel...|[and, pleas, tell...|[pleas, tell, mom...|[-0.0806874127204...|[-0.0806874127204...|[0.92665022521186...|[0.73041203416501...|       0.0|\n",
      "|    0|\"\"\"Angry Birds?\"\"...|[angri, bird, u, ...|[angri, bird, u, ...|[-0.0954395830631...|[-0.0954395830631...|[0.83370367517800...|[0.68286281261915...|       0.0|\n",
      "|    0|\"\"\"Any objections...|[ani, object, fuc...|[ani, object, fuc...|[-0.0658665656721...|[-0.0658665656721...|[0.54636931545291...|[0.58363194530577...|       0.0|\n",
      "|    0|\"\"\"Anyway here's ...|[anywai, here, st...|[anywai, stairwai...|[0.01289488588060...|[0.01289488588060...|[0.68411251201754...|[0.60939820120988...|       0.0|\n",
      "|    0|\"\"\"Aren't you a C...|[ar, nt, you, a, ...|[ar, nt, christia...|[-0.1448144886775...|[-0.1448144886775...|[-0.1662768720381...|[0.29502407006247...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_featurized = feature_model.transform(test)\n",
    "preds = model.transform(test_featurized)\n",
    "preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = preds.select('comment', 'label', 'prediction').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!RemindMe 1 week\\n!RemindMe 2 days</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!Remindme 2 weeks\\n!Remindme 2 weeks</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!SH!TPOST!: All those poor USA Streamers\\nNow ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\"\"**FUCK** Cloud\"\" - Link main\"\\nYep, that's ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"\"\"*Komrad\\n\"*\"\"Those were just pro-USA rebels...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label  prediction\n",
       "0                 !RemindMe 1 week\\n!RemindMe 2 days      0         0.0\n",
       "1               !Remindme 2 weeks\\n!Remindme 2 weeks      0         0.0\n",
       "2  !SH!TPOST!: All those poor USA Streamers\\nNow ...      0         1.0\n",
       "3  \"\"\"**FUCK** Cloud\"\" - Link main\"\\nYep, that's ...      0         0.0\n",
       "4  \"\"\"*Komrad\\n\"*\"\"Those were just pro-USA rebels...      0         1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred 0</th>\n",
       "      <th>pred 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true 0</th>\n",
       "      <td>13928</td>\n",
       "      <td>3296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true 1</th>\n",
       "      <td>8415</td>\n",
       "      <td>4225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pred 0  pred 1\n",
       "true 0   13928    3296\n",
       "true 1    8415    4225"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics as skmetrics\n",
    "pd.DataFrame(\n",
    "    data=skmetrics.confusion_matrix(pred_df['label'], pred_df['prediction']),\n",
    "    columns=['pred ' + l for l in ['0','1']],\n",
    "    index=['true ' + l for l in ['0','1']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.81      0.70     17224\n",
      "           1       0.56      0.33      0.42     12640\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     29864\n",
      "   macro avg       0.59      0.57      0.56     29864\n",
      "weighted avg       0.60      0.61      0.58     29864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(skmetrics.classification_report(pred_df['label'], pred_df['prediction'], \n",
    "                                      target_names=['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
