{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.7.72:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbdec058978>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some basic imports\n",
    "import sparknlp\n",
    "sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('Spark NLP with OCR') \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.jars.repositories\", \"http://repo.spring.io/plugins-release\") \\\n",
    "    .config(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:2.2.1,com.johnsnowlabs.nlp:spark-nlp-ocr_2.11:2.2.1,javax.media.jai:com.springsource.javax.media.jai.core:1.1.3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text='would have been a liberation, a joy, and a fiesta.\\nHe sensed that had he been able to choose or\\ndream his death that night, this is the death he\\nwould have dreamed or chosen.\\n\\nDahlmann firmly grips the knife, which he\\nmay have no idea how to manage, and steps out\\ninto the plains.\\n', pagenum=0, method='image', noiselevel=0.0, confidence=0.0, positions=None, filename='file:/home/jose/Documents/presentation-oreilly-ai/notebook/immortal_image.pdf'),\n",
       " Row(text='The Aleph\\n(1949)\\n', pagenum=0, method='image', noiselevel=0.0, confidence=0.0, positions=None, filename='file:/home/jose/Documents/presentation-oreilly-ai/notebook/immortal_image.pdf'),\n",
       " Row(text='The Immortal\\n', pagenum=0, method='image', noiselevel=0.0, confidence=0.0, positions=None, filename='file:/home/jose/Documents/presentation-oreilly-ai/notebook/immortal_image.pdf'),\n",
       " Row(text='Solomon saith: There is no new thing upon\\nthe earth. So that as Plato had an imagination,\\nthat all knowledge was but remembrance; so\\nSolomon giveth his sentence, that all novelty is\\nbut oblivion.\\n\\nFrancis Bacon: Essays, LVIII\\n', pagenum=0, method='image', noiselevel=0.0, confidence=0.0, positions=None, filename='file:/home/jose/Documents/presentation-oreilly-ai/notebook/immortal_image.pdf'),\n",
       " Row(text='In London, 1n early June of the year 19209,\\n', pagenum=0, method='image', noiselevel=0.0, confidence=0.0, positions=None, filename='file:/home/jose/Documents/presentation-oreilly-ai/notebook/immortal_image.pdf')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do OCR\n",
    "from sparknlp.ocr import OcrHelper\n",
    "ocrHelper = OcrHelper()\n",
    "ocrHelper.setPreferredMethod(\"image\")\n",
    "\n",
    "\n",
    "#If you do this locally you can use file:/// or hdfs:/// if the files are hosted in Hadoop\n",
    "dataset = ocrHelper.createDataset(spark, './immortal_image.pdf')\n",
    "dataset.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 144.3 MB\n",
      "[OK!]\n",
      "spellcheck_norvig download started this may take some time.\n",
      "Approximate size to download 4.2 MB\n",
      "[OK!]\n",
      "ner_dl_contrib download started this may take some time.\n",
      "Approximate size to download 13.6 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "# create pipeline, (almost) from scratch\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "embeddings = WordEmbeddingsModel().pretrained() \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "spell = NorvigSweetingModel.pretrained() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"spell\")\n",
    "\n",
    "ner_dl = NerDLModel().pretrained() \\\n",
    "    .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"ner_dl\")\n",
    "\n",
    "ner_converter = NerConverter() \\\n",
    "    .setInputCols([\"document\", \"token\", \"ner_dl\"]) \\\n",
    "    .setOutputCol(\"ner_chunk\")\n",
    "    \n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"document\", \"token\", \"ner_dl\", \"ner_chunk\"]) \\\n",
    "    .setIncludeMetadata(True)    \n",
    "    \n",
    "pipeline = Pipeline(stages = [documentAssembler, tokenizer, embeddings, spell, ner_dl, ner_converter])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(annotatorType='chunk', begin=0, end=6, result='Solomon', metadata={'sentence': '0', 'chunk': '0', 'entity': 'PER'}, embeddings=[], sentence_embeddings=[]) \n",
      "\n",
      "Row(annotatorType='chunk', begin=64, end=68, result='Plato', metadata={'sentence': '0', 'chunk': '1', 'entity': 'PER'}, embeddings=[], sentence_embeddings=[]) \n",
      "\n",
      "Row(annotatorType='chunk', begin=133, end=139, result='Solomon', metadata={'sentence': '0', 'chunk': '2', 'entity': 'PER'}, embeddings=[], sentence_embeddings=[]) \n",
      "\n",
      "Row(annotatorType='chunk', begin=197, end=209, result='Francis Bacon', metadata={'sentence': '0', 'chunk': '3', 'entity': 'PER'}, embeddings=[], sentence_embeddings=[]) \n",
      "\n",
      "Row(annotatorType='chunk', begin=220, end=224, result='LVIII', metadata={'sentence': '0', 'chunk': '4', 'entity': 'ORG'}, embeddings=[], sentence_embeddings=[]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = pipeline.fit(dataset).transform(dataset).select('ner_chunk').collect()[3][:10] # 3 and 4\n",
    "for r in rows[0]:\n",
    "    print(str(r), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_dl_fast download started this may take some time.\n",
      "Approx size to download 167.4 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# complete pretrained pipelines\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "pipeline = PretrainedPipeline('explain_document_dl_fast', 'en')\n",
    "annotations = pipeline.annotate(\"Harry Potter is a graet muvie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Harry', 'I-PER'),\n",
       " ('Potter', 'I-PER'),\n",
       " ('is', 'O'),\n",
       " ('a', 'O'),\n",
       " ('great', 'O'),\n",
       " ('movie', 'O')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked = annotations['checked']\n",
    "ner = annotations['ner']\n",
    "list(zip(checked, ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
