{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nlp.johnsnowlabs.com/assets/images/logo.png\" width=\"180\" height=\"50\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Text Matching\n",
    "\n",
    "In the following example, we walk-through our straight forward Text Matcher Annotator.\n",
    "\n",
    "This annotator will take a list of sentences from a text file and look them up in the given target dataset.\n",
    "\n",
    "This annotator is an Annotator Model and hence does not require training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark `2.4` and Spark NLP `2.0.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Call necessary imports and set the resource path to read local data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "\n",
    "#Setting location of resource Directory\n",
    "resource_path= \"../../../src/test/resources/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load SparkSession if not already there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create appropriate annotators. We are using Sentence Detection and Tokenizing the sentence. The Finisher will clean the annotations and exclude the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "extractor = TextMatcher()\\\n",
    "  .setEntities(\"entities.txt\")\\\n",
    "  .setInputCols([\"token\", \"sentence\"])\\\n",
    "  .setOutputCol(\"entites\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"entites\"]) \\\n",
    "    .setIncludeMetadata(False) \\\n",
    "    .setCleanAnnotations(True)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    documentAssembler,\n",
    "    sentenceDetector,\n",
    "    tokenizer,\n",
    "    extractor,\n",
    "    finisher\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Load the input data to be annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-21 18:12:44--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment.parquet.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.162.13\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.162.13|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 76127532 (73M) [application/zip]\n",
      "Saving to: ‘/tmp/sentiment.parquet.zip’\n",
      "\n",
      "sentiment.parquet.z 100%[===================>]  72.60M  3.13MB/s    in 18s     \n",
      "\n",
      "2019-03-21 18:13:02 (4.08 MB/s) - ‘/tmp/sentiment.parquet.zip’ saved [76127532/76127532]\n",
      "\n",
      "Archive:  /tmp/sentiment.parquet.zip\n",
      "replace /tmp/sentiment.parquet/.part-00002-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet.crc? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "! rm /tmp/sentiment.parquet.zip\n",
    "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment.parquet.zip -P /tmp\n",
    "! unzip /tmp/sentiment.parquet.zip -d /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+\n",
      "|itemid|sentiment|                text|\n",
      "+------+---------+--------------------+\n",
      "|799033|        0|@FrankomQ8 What's...|\n",
      "|799034|        1|@FranKoUK guitar ...|\n",
      "|799035|        0|@frankparenteau u...|\n",
      "|799036|        1|@frankparenteau w...|\n",
      "|799037|        1|@FrankPatris dude...|\n",
      "|799038|        0|@FrankRamblings a...|\n",
      "|799039|        1|@frankroberts  ni...|\n",
      "|799040|        0|@frankroberts ur ...|\n",
      "|799041|        1|@FrankS Breaking ...|\n",
      "|799042|        1|@frankschultelad ...|\n",
      "|799043|        0|@frankshorter Wol...|\n",
      "|799044|        0|@franksting - its...|\n",
      "|799045|        1|@franksting Ha! D...|\n",
      "|799046|        1|@franksting yeah,...|\n",
      "|799047|        1|@franksting yes, ...|\n",
      "|799048|        1|@FrankSylar arn't...|\n",
      "|799049|        1|    @frankules WO ? |\n",
      "|799050|        0|@frankwkelly I'm ...|\n",
      "|799051|        1|@FrankXSalinas Th...|\n",
      "|799052|        1|@frankybhoy93 tha...|\n",
      "+------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark. \\\n",
    "        read. \\\n",
    "        parquet(\"/tmp/sentiment.parquet\"). \\\n",
    "        limit(1000).cache()\n",
    "data.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Running the fit for sentence detection and tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting\n",
      "Fitting is ended\n"
     ]
    }
   ],
   "source": [
    "print(\"Start fitting\")\n",
    "model = pipeline.fit(data)\n",
    "print(\"Fitting is ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Runing the transform on data to do text matching. It will append a new coloumns with matched entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+----------------+\n",
      "|itemid|sentiment|                text|finished_entites|\n",
      "+------+---------+--------------------+----------------+\n",
      "|799033|        0|@FrankomQ8 What's...|              []|\n",
      "|799034|        1|@FranKoUK guitar ...|              []|\n",
      "|799035|        0|@frankparenteau u...|              []|\n",
      "|799036|        1|@frankparenteau w...|              []|\n",
      "|799037|        1|@FrankPatris dude...|              []|\n",
      "|799038|        0|@FrankRamblings a...|              []|\n",
      "|799039|        1|@frankroberts  ni...|              []|\n",
      "|799040|        0|@frankroberts ur ...|              []|\n",
      "|799041|        1|@FrankS Breaking ...|              []|\n",
      "|799042|        1|@frankschultelad ...|              []|\n",
      "|799043|        0|@frankshorter Wol...|              []|\n",
      "|799044|        0|@franksting - its...|              []|\n",
      "|799045|        1|@franksting Ha! D...|              []|\n",
      "|799046|        1|@franksting yeah,...|              []|\n",
      "|799047|        1|@franksting yes, ...|              []|\n",
      "|799048|        1|@FrankSylar arn't...|              []|\n",
      "|799049|        1|    @frankules WO ? |              []|\n",
      "|799050|        0|@frankwkelly I'm ...|              []|\n",
      "|799051|        1|@FrankXSalinas Th...|              []|\n",
      "|799052|        1|@frankybhoy93 tha...|              []|\n",
      "+------+---------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted = model.transform(data)\n",
    "extracted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. The model could be saved locally and reloaded to run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.write().overwrite().save(\"./extractor_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+----------------+\n",
      "|itemid|sentiment|                text|finished_entites|\n",
      "+------+---------+--------------------+----------------+\n",
      "|799033|        0|@FrankomQ8 What's...|              []|\n",
      "|799034|        1|@FranKoUK guitar ...|              []|\n",
      "|799035|        0|@frankparenteau u...|              []|\n",
      "|799036|        1|@frankparenteau w...|              []|\n",
      "|799037|        1|@FrankPatris dude...|              []|\n",
      "|799038|        0|@FrankRamblings a...|              []|\n",
      "|799039|        1|@frankroberts  ni...|              []|\n",
      "|799040|        0|@frankroberts ur ...|              []|\n",
      "|799041|        1|@FrankS Breaking ...|              []|\n",
      "|799042|        1|@frankschultelad ...|              []|\n",
      "|799043|        0|@frankshorter Wol...|              []|\n",
      "|799044|        0|@franksting - its...|              []|\n",
      "|799045|        1|@franksting Ha! D...|              []|\n",
      "|799046|        1|@franksting yeah,...|              []|\n",
      "|799047|        1|@franksting yes, ...|              []|\n",
      "|799048|        1|@FrankSylar arn't...|              []|\n",
      "|799049|        1|    @frankules WO ? |              []|\n",
      "|799050|        0|@frankwkelly I'm ...|              []|\n",
      "|799051|        1|@FrankXSalinas Th...|              []|\n",
      "|799052|        1|@frankybhoy93 tha...|              []|\n",
      "+------+---------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import  Pipeline\n",
    "\n",
    "sameModel = PipelineModel.read().load(\"./extractor_model\")\n",
    "\n",
    "sameModel.transform(data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
