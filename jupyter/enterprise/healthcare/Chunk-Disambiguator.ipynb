{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nlp.johnsnowlabs.com/assets/images/logo.png\" width=\"180\" height=\"50\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk-Disambiguator - version 2.4.0\n",
    "\n",
    "## Example for people disambiguator\n",
    "\n",
    "A typical use case in NLP tasks is to be able to, once we have identified with a Named Entity Recognition model that a given chunk is refering to a person to be able to link that chunk to a particular person using an external source as the Wikipedia.\n",
    "\n",
    "This is fundamentally a multiclass classification problem where the classes are all the wikipedia entries referring to persons and the entity to be disambiguated is the piece of original content we know is referring to a person. That is why this kind of annotators are named disambiguators.\n",
    "\n",
    "Lets imagine we have this sentence:\n",
    "\n",
    "<div style=\"border:2px solid #747474; background-color: #e3e3e3; margin: 5px; padding: 10px\"> \n",
    "Ronald Reagan was a president of the United States during the 80s.<br>\n",
    "</div>\n",
    "\n",
    "SparkNLP Enterprise provides with pipeline functionalities that allow to identify which words are referring to persons (Ronald Reagan) and to link each of those references to the especific entry in the wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Prepare the environment\n",
    "\n",
    "#### Install OpenSource spark-nlp and pyspark pip packages\n",
    "As a first step we import the required python dependences including some sparknlp components.\n",
    "\n",
    "Be sure that you have the required python libraries (pyspark 2.4.4, spark-nlp 2.4.0) by running <code>pip list</code>. Check that the versions are correct.\n",
    "\n",
    "If some of them is missing you can run:\n",
    "\n",
    "<code>pip install --ignore-installed pyspark==2.4.4</code><br>\n",
    "<code>pip install --ignore-installed spark-nlp==2.4.0</code><br>\n",
    "\n",
    "The --ignore-installed parameter is to overwrite your previous pip package version if already installed.\n",
    "\n",
    "<i>*If this cell fails means you have not propertly setup the required environment. Please check the pre-requisites guideline at http://www.johnsnowlabs.com</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Licensed Sparl-NLP package\n",
    "\n",
    "We will use also some Spark-NLP enterprise functionalities contained in the spark-nlp-jsl package.\n",
    "\n",
    "You can check that spark-nlp-jsl is installed by running <code>pip install</code>. Check that version installed is 2.4.0\n",
    "\n",
    "If it is not then you need to install it by using:\n",
    "\n",
    "<code>pip install spark-nlp-jsl==2.4.0 --extra-index-url https://pypi.johnsnowlabs.com/##### --ignore-installed</code>\n",
    "\n",
    "The ##### is a secret url, if you have not received it please contact us at info@johnsnowlabs.com.\n",
    "\n",
    "<i>*If the next cell fails means your licensed enterprise version is not propertly installed so please check the pre-requisites guideline at http://www.johnsnowlabs.com/</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp_jsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup credentials to private JohnSnowLabs models repository with AWS-CLI\n",
    "\n",
    "Now is time to configure Spark-NLP in order to access private JohnSnowLabs models repository. This access is done via Amazon aws command line interface (AWSCLI).\n",
    "\n",
    "Instructions about how to install awscli are available at:\n",
    "\n",
    "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n",
    "\n",
    "Make sure you configure your credentials with aws configure following the instructions at:\n",
    "\n",
    "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html\n",
    "\n",
    "Please substitute the ACCESS_KEY and SECRET_KEY with the credentials you have recived. If you need your credentials contact us at info@johnsnowlabs.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup your license number in order to have access to private JohnSnowLabs models repository\n",
    "\n",
    "You need to setup the SPARK_NLP_LICENSE environment variable to the license number provided to you. If you need your license credentials contact us at info@johnsnowlabs.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_NLP_LICENSE']='#########'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Spark session\n",
    "\n",
    "The following will initialize the spark session in case you have run the jupyter notebook directly. If you have started the notebook using pyspark this cell is just ignored.\n",
    "\n",
    "Initializing the spark session takes some seconds (usually less than 1 minute) as the jar from the server needs to be loaded.\n",
    "\n",
    "The ####### is a secret code required to run the licensed version 2.4.0, if you have not received it please contact us at info@johnsnowlabs.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp_jsl.start(\"########\") # Secret code provided as part of the license"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: De-identification pipeline generation\n",
    "\n",
    "In Spark-NLP annotating NLP happens through pipelines. Pipelines are made out of various Annotator steps. In our case the architecture of the Clinical Named Entity Recognition pipeline will be:\n",
    "\n",
    "* DocumentAssembler (text -> document)\n",
    "* SentenceDetector (document -> sentence)\n",
    "* Tokenizer (sentence -> token)\n",
    "* WordEmbeddingsModel ([sentence, token] -> embeddings)\n",
    "* NerDLModel (deidentify_dl) ([sentence, token, embeddings] -> ner)\n",
    "* NerConverter ([sentence, token, ner] -> ner_chunk)\n",
    "* DisambiguatorModel ([ner_chunk, embeddings] -> deidentified\n",
    "\n",
    "So from a text we end having a link to the wikipedia urls for the persons referenced in the document.\n",
    "\n",
    "We will use a pretrained model (NerDLModel deidentify) that leveraging in a language model encoded in word embeddings (embeddings) is able to recognize tokens that are naming persons, organizations and other. \n",
    "\n",
    "Then we transform its output (ner) into chunks (ner_chunk) that are then used by another pretrained annotator (DisambiguatorModel). The disambiguator will select the most relevant wikipedia entry for those chunks naming persons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotator that transforms a text column from dataframe into an Annotation ready for NLP\n",
    "\n",
    "document = DocumentAssembler()\\\n",
    ".setInputCol(\"text\")\\\n",
    ".setOutputCol(\"document\")\n",
    "\n",
    "# Sentence Detector annotator, processes various sentences per line\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"sentence\")\n",
    "\n",
    "# Tokenizer splits words in a relevant format for NLP\n",
    "\n",
    "token = Tokenizer()\\\n",
    ".setInputCols([\"sentence\"])\\\n",
    ".setOutputCol(\"token\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth annotator in the pipeline is \"WordEmbeddingsModel\". We will download a pretrained model available <a href='https://nlp.stanford.edu/projects/glove/'>freely available</a> named \"globe_100d\".\n",
    "\n",
    "When running this cell your are advised to be patient. \n",
    "\n",
    "First time you call this pretrained model it needs to be downloaded in your local and it takes a while (depending on your internet connection).\n",
    "\n",
    "The size is about 145Mb and will be saved typically in your home folder as\n",
    "\n",
    "    ~HOMEFOLDER/cached_models/glove_100d_en_2.4.0_2.4_1579690104032\n",
    "\n",
    "Next times you call it the model is loaded from your cached copy but even in that case it needs to be indexed each time so expect waiting up to 1 minutes (depending on your machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "wordEmbeddings = WordEmbeddingsModel.pretrained(\"glove_100d\", \"en\")\\\n",
    ".setInputCols(\"sentence\", \"token\")\\\n",
    ".setOutputCol(\"word_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now will download another freely available SparknLP pretrained model (ner_dl) that consists in a Named Entity Resolver based on a DeepLearning architecture that uses as input Glove100d embeddings (previously loaded in our pipeline). Its size is about 13Mb and will be typically stored in your local machine as:\n",
    "\n",
    "    ~HOMEFOLDER/cached_models/ner_dl_en_2.4.0_2.4_1580251789753\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_dl download started this may take some time.\n",
      "Approximate size to download 13.5 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "ner = NerDLModel.pretrained()\\\n",
    ".setInputCols(\"sentence\", \"token\", \"word_embeddings\")\\\n",
    ".setOutputCol(\"ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next pipeline's component is the NerConverter that will filter only for person entities (therefor the <code>.setWhiteList([\"PER])</code>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerConverter = NerConverter()\\\n",
    ".setInputCols(\"sentence\", \"token\", \"ner\")\\\n",
    ".setOutputCol(\"ner_chunk\")\\\n",
    ".setWhiteList([\"PER\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we load the pretrained people disambiguator. This pretrained model is licensed so if this cell fails is because you have not setup the AWS-CLI or SPARK_NLP_LICENSE environmental variable.\n",
    "\n",
    "Contact us at info@johnsnowlabs.com if you have not received the required credentials and license key.\n",
    "\n",
    "The \"people_disambiguator\" model size is about 54Mb and will be typically saved in your local system as:\n",
    "\n",
    "    ~HOMEFOLDER/cached_models/people_disambiguator_en_2.3.4_2.4_1574806205059"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people_disambiguator download started this may take some time.\n",
      "Approximate size to download 54.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "disambiguator = sparknlp_jsl.annotator.DisambiguatorModel.pretrained('people_disambiguator', 'en', 'clinical/models')\\\n",
    ".setInputCols(\"ner_chunk\", \"word_embeddings\")\\\n",
    ".setOutputCol(\"disambiguation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2 Defining the stages of the pipeline\n",
    "Now that we have created all the components of our pipeline, lets put all them together into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages([\n",
    "    document,\n",
    "    sentenceDetector,\n",
    "    token,\n",
    "    wordEmbeddings,\n",
    "    ner,\n",
    "    nerConverter,\n",
    "    disambiguator\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Fit the pipeline with some data\n",
    "Lest now see how our Deidientification pipeline works with some data. We will use the following sentence naming Ronald Reagan:\n",
    "\n",
    "<div style=\"border:2px solid #747474; background-color: #e3e3e3; margin: 5px; padding: 10px\"> \n",
    "Ronald Reagan was a president of the United States during the 80s.<br>\n",
    "</div>\n",
    "\n",
    "We will create a Spark DataFrame containing the only line of this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------+\n",
      "|id |text                                                              |\n",
      "+---+------------------------------------------------------------------+\n",
      "|1  |Ronald Reagan was a president of the United States during the 80s.|\n",
      "+---+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.createDataFrame([\n",
    "    [1, \"Ronald Reagan was a president of the United States during the 80s.\"]\n",
    "]).toDF('id', 'text')\n",
    "\n",
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a model by fitting our pipeline to our content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And will apply the model transforming our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our pipeline are stored in <code>output</code> a Spark DataFrame, so lets show some relevant columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+\n",
      "|         result|              result|            metadata|\n",
      "+---------------+--------------------+--------------------+\n",
      "|[Ronald Reagan]|[http://en.wikipe...|[[chunk -> Ronald...|\n",
      "+---------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.select('ner_chunk.result', 'disambiguation.result', 'disambiguation.metadata').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of Spark is not especially appealing so for the sake of this demo we can extract just the first row of that dataframe, extract the relevant pieces of information and show them in a prettier html format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><table style='font-size: 1.0em; border: 1px solid'><tr style='border: 1px solid'><td style='text-align: left'>Original sentence:</td><td></td></tr><tr style='border: 1px solid'><td>Ronald Reagan was a president of the United States during the 80s.</td><td></td></tr><tr style='border: 1px solid'><td style='text-align: left'>Person Chunk: Ronald Reagan</td><td></td><tr style='border: 1px solid'><td style='text-align: center'>URL candidate</td><td style='text-align: center'>Score</td></tr><tr><td style='text-align: center; border: 1px solid'>http://en.wikipedia.org/wiki/Ronald_Reagan</td><td style='text-align: left; border: 1px solid'>0.9672</td></tr><tr><td style='text-align: center; border: 1px solid'>http://en.wikipedia.org/wiki/Nancy_Reagan</td><td style='text-align: left; border: 1px solid'>0.9551</td></tr><tr><td style='text-align: center; border: 1px solid'>http://en.wikipedia.org/wiki/Ronald_Hines</td><td style='text-align: left; border: 1px solid'>0.9398</td></tr><tr><td style='text-align: center; border: 1px solid'>http://en.wikipedia.org/wiki/Ronald_Brittain</td><td style='text-align: left; border: 1px solid'>0.9376</td></tr><tr><td style='text-align: center; border: 1px solid'>http://en.wikipedia.org/wiki/Ronald_Millar</td><td style='text-align: left; border: 1px solid'>0.9373</td></tr></table></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We extract the NER chunk string:\n",
    "chunk_token = output.select('ner_chunk.result').take(1)[0]['result'][0]\n",
    "\n",
    "# We extract the URLs suggested by the disambiguator\n",
    "disambiguator_urls = output.select('disambiguation.result').take(1)\n",
    "dis_urls = [x.strip() for x in disambiguator_urls[0]['result'][0].split(\",\")]\n",
    "\n",
    "# We extract the scores suggested by the disambiguator\n",
    "disambiguator_scores = output.select('disambiguation.metadata').take(1)\n",
    "dis_scores = [float(x.strip()) for x in disambiguator_scores[0]['metadata'][0]['scores'].split(\",\")]\n",
    "\n",
    "# Now we print all the relevant information in a HTML table\n",
    "html_output = \"<center><table style='font-size: 1.0em; border: 1px solid'>\"\n",
    "html_output += \"<tr style='border: 1px solid'><td style='text-align: left'>Original sentence:</td><td></td></tr>\"\n",
    "html_output += \"<tr style='border: 1px solid'><td>Ronald Reagan was a president of the United States during the 80s.</td><td></td></tr>\"\n",
    "\n",
    "html_output += \"<tr style='border: 1px solid'><td style='text-align: left'>Person Chunk: \" + chunk_token + \"</td><td></td>\"\n",
    "html_output += \"<tr style='border: 1px solid'><td style='text-align: center'>URL candidate</td><td style='text-align: center'>Score</td></tr>\"\n",
    "for this_index in range(len(dis_urls)):\n",
    "    html_output += \"<tr><td style='text-align: center; border: 1px solid'>\" + dis_urls[this_index] + \"</td><td style='text-align: left; border: 1px solid'>\" + str(dis_scores[this_index]) + \"</td></tr>\"\n",
    "html_output += \"</table></center>\"\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how our pipeline has identified \"Ronald Reagan\" as a chunk referring to a person. For this chunk it calculates a score that indicates the likelihood of each wikipedia article to belong to the person named in the chunk.\n",
    "\n",
    "In our example http://en.wikipedia.org/wiki/Ronald_Reagan gets the highest score (0.9672) followed by the entry for Nancy Reagan (0.9551).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 with LightPipelines\n",
    "\n",
    "Once you have created a model by fitting a pipeline with some data you can leverage the use of LightPipelines, faster and easier to use for testing or real-time queries.\n",
    "\n",
    "Lets created a light_pipeline from our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_pipeline = LightPipeline(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the pipeline for a new slightly different sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_data = light_pipeline.annotate(\"Nancy Reagan was the spouse of one president of the United States.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are stored in a python dictionary, lets check the sentence identified by our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nancy Reagan was the spouse of one president of the United States.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light_data['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the chunk identified as a person name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nancy Reagan']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light_data['ner_chunk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how the list of urls that our people_desambiguator suggests has now changed, being Nancy Reagan wikipedia entry the one with the highest score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://en.wikipedia.org/wiki/Nancy_Reagan',\n",
       " 'http://en.wikipedia.org/wiki/Ronald_Reagan',\n",
       " 'http://en.wikipedia.org/wiki/Nancy_McIntosh',\n",
       " 'http://en.wikipedia.org/wiki/Nancy_Nevinson',\n",
       " 'http://en.wikipedia.org/wiki/Nancy_Guild']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.strip() for x in light_data['disambiguation'][0].split(\",\")]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
