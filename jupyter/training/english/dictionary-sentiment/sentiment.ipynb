{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nlp.johnsnowlabs.com/assets/images/logo.png\" width=\"180\" height=\"50\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based Sentiment Analysis\n",
    "\n",
    "In the following example, we walk-through a simple use case for our straight forward SentimentDetector annotator.\n",
    "\n",
    "This annotator will work on top of a list of labeled sentences which can have any of the following features\n",
    "    \n",
    "    positive\n",
    "    negative\n",
    "    revert\n",
    "    increment\n",
    "    decrement\n",
    "\n",
    "Each of these sentences will be used for giving a score to text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Call necessary imports and set the resource path to read local data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import array_contains\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import RegexRule\n",
    "from sparknlp.base import DocumentAssembler, Finisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load SparkSession if not already there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version\n",
      "2.2.1\n",
      "Apache Spark version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp \n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version\")\n",
    "sparknlp.version()\n",
    "print(\"Apache Spark version\")\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-16 21:54:20--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment.parquet.zip\n",
      "R'esolution de s3.amazonaws.com (s3.amazonaws.com)... 52.217.37.86\n",
      "Connexion `a s3.amazonaws.com (s3.amazonaws.com)|52.217.37.86|:443... connect'e.\n",
      "requ^ete HTTP transmise, en attente de la r'eponse... 200 OK\n",
      "Taille : 76127532 (73M) [application/zip]\n",
      "Sauvegarde en : << /tmp/sentiment.parquet.zip >>\n",
      "\n",
      "sentiment.parquet.z 100%[===================>]  72.60M   855KB/s    ds 2m 23s  \n",
      "\n",
      "2019-07-16 21:56:44 (520 KB/s) - << /tmp/sentiment.parquet.zip >> sauvegard'e [76127532/76127532]\n",
      "\n",
      "--2019-07-16 21:56:44--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/lemma-corpus-small/lemmas_small.txt\n",
      "R'esolution de s3.amazonaws.com (s3.amazonaws.com)... 52.216.129.61\n",
      "Connexion `a s3.amazonaws.com (s3.amazonaws.com)|52.216.129.61|:443... connect'e.\n",
      "requ^ete HTTP transmise, en attente de la r'eponse... 200 OK\n",
      "Taille : 189437 (185K) [text/plain]\n",
      "Sauvegarde en : << /tmp/lemmas_small.txt >>\n",
      "\n",
      "lemmas_small.txt    100%[===================>] 185.00K   212KB/s    ds 0.9s    \n",
      "\n",
      "2019-07-16 21:56:46 (212 KB/s) - << /tmp/lemmas_small.txt >> sauvegard'e [189437/189437]\n",
      "\n",
      "--2019-07-16 21:56:46--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/default-sentiment-dict.txt\n",
      "R'esolution de s3.amazonaws.com (s3.amazonaws.com)... 52.216.129.61\n",
      "Connexion `a s3.amazonaws.com (s3.amazonaws.com)|52.216.129.61|:443... connect'e.\n",
      "requ^ete HTTP transmise, en attente de la r'eponse... 200 OK\n",
      "Taille : 289 [text/plain]\n",
      "Sauvegarde en : << /tmp/default-sentiment-dict.txt >>\n",
      "\n",
      "default-sentiment-d 100%[===================>]     289  --.-KB/s    ds 0s      \n",
      "\n",
      "2019-07-16 21:56:47 (5.10 MB/s) - << /tmp/default-sentiment-dict.txt >> sauvegard'e [289/289]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! rm /tmp/sentiment.parquet.zip\n",
    "! rm -rf /tmp/sentiment.parquet\n",
    "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment.parquet.zip -P /tmp\n",
    "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/lemma-corpus-small/lemmas_small.txt -P /tmp\n",
    "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/default-sentiment-dict.txt -P /tmp    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /tmp/sentiment.parquet.zip\n",
      "   creating: /tmp/sentiment.parquet/\n",
      "  inflating: /tmp/sentiment.parquet/.part-00002-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet.crc  \n",
      "  inflating: /tmp/sentiment.parquet/part-00002-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet  \n",
      "  inflating: /tmp/sentiment.parquet/part-00003-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet  \n",
      "  inflating: /tmp/sentiment.parquet/.part-00000-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet.crc  \n",
      "  inflating: /tmp/sentiment.parquet/part-00001-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet  \n",
      " extracting: /tmp/sentiment.parquet/_SUCCESS  \n",
      "  inflating: /tmp/sentiment.parquet/.part-00003-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet.crc  \n",
      "  inflating: /tmp/sentiment.parquet/part-00000-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet  \n",
      "  inflating: /tmp/sentiment.parquet/.part-00001-08092d15-dd8c-40f9-a1df-641a1a4b1698.snappy.parquet.crc  \n"
     ]
    }
   ],
   "source": [
    "! unzip /tmp/sentiment.parquet.zip -d /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+\n",
      "|itemid|sentiment|                text|\n",
      "+------+---------+--------------------+\n",
      "|799033|        0|@FrankomQ8 What's...|\n",
      "|799034|        1|@FranKoUK guitar ...|\n",
      "|799035|        0|@frankparenteau u...|\n",
      "|799036|        1|@frankparenteau w...|\n",
      "|799037|        1|@FrankPatris dude...|\n",
      "|799038|        0|@FrankRamblings a...|\n",
      "|799039|        1|@frankroberts  ni...|\n",
      "|799040|        0|@frankroberts ur ...|\n",
      "|799041|        1|@FrankS Breaking ...|\n",
      "|799042|        1|@frankschultelad ...|\n",
      "|799043|        0|@frankshorter Wol...|\n",
      "|799044|        0|@franksting - its...|\n",
      "|799045|        1|@franksting Ha! D...|\n",
      "|799046|        1|@franksting yeah,...|\n",
      "|799047|        1|@franksting yes, ...|\n",
      "|799048|        1|@FrankSylar arn't...|\n",
      "|799049|        1|    @frankules WO ? |\n",
      "|799050|        0|@frankwkelly I'm ...|\n",
      "|799051|        1|@FrankXSalinas Th...|\n",
      "|799052|        1|@frankybhoy93 tha...|\n",
      "+------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark. \\\n",
    "        read. \\\n",
    "        parquet(\"/tmp/sentiment.parquet\"). \\\n",
    "        limit(10000).cache()\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create appropriate annotators. We are using Sentence Detection, Tokenizing the sentences, and find the lemmas of those tokens. The Finisher will only output the Sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "lemmatizer = Lemmatizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"lemma\") \\\n",
    "    .setDictionary(\"/tmp/lemmas_small.txt\", key_delimiter=\"->\", value_delimiter=\"\\t\")\n",
    "        \n",
    "sentiment_detector = SentimentDetector() \\\n",
    "    .setInputCols([\"lemma\", \"sentence\"]) \\\n",
    "    .setOutputCol(\"sentiment_score\") \\\n",
    "    .setDictionary(\"/tmp/default-sentiment-dict.txt\", \",\")\n",
    "    \n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"sentiment_score\"]) \\\n",
    "    .setOutputCols([\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Train the pipeline, which is only being trained from external resources, not from the dataset we pass on. The prediction runs on the target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, lemmatizer, sentiment_detector, finisher])\n",
    "model = pipeline.fit(data)\n",
    "result = model.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. filter the finisher output, to find the positive sentiment lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|itemid|sentiment |text                                                                                                                                     |\n",
      "+------+----------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|799033|[positive]|@FrankomQ8 What's in it? I'm starving                                                                                                    |\n",
      "|799034|[positive]|@FranKoUK guitar lessons soundsss good  when?  xx                                                                                        |\n",
      "|799035|[positive]|@frankparenteau used and abused, huh? i do feel like that sometimes, especially when clients ignore my invoices.                         |\n",
      "|799036|[positive]|@frankparenteau well, with itunes he gets more hits w/o promo, probably. eh...                                                           |\n",
      "|799037|[positive]|@FrankPatris dude have you season 5? i finished 16 eoisodes of season 4 na                                                               |\n",
      "|799038|[positive]|@FrankRamblings and I CAN listen to them on Itunes, so it's not a corrupted file problem                                                 |\n",
      "|799039|[positive]|@frankroberts  nice.  this has happened to me like 20 times. lol.  pace yourself.                                                        |\n",
      "|799040|[positive]|@frankroberts ur on top of ur twitter game! thanks hun. like you, I wish I updated more. Dig your writings as well. Sad news RE: Octavia |\n",
      "|799041|[positive]|@FrankS Breaking Bad: eher Drama mit Comedy, Entourage: Single Camera Comedy, muss man auf jeden Fall gesehen haben                      |\n",
      "|799042|[positive]|@frankschultelad Good morning..Thanks!                                                                                                   |\n",
      "+------+----------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.where(array_contains(result.sentiment, \"positive\")).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
